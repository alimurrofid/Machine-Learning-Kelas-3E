{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lpKmZSM4HjZG"
      },
      "outputs": [],
      "source": [
        "# import library\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYWOGsZhHjZS",
        "outputId": "6f722cb0-f871-4fb0-bd47-3d0bdf65d724"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 1s 1us/step\n"
          ]
        }
      ],
      "source": [
        "# download dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sdnARxoEHjZU",
        "outputId": "ac50c4e6-0330-4482-8d91-c1d6361ddad4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ],
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yLp6EEQHjZa",
        "outputId": "691ad631-9a3b-400a-adca-c729a5d38e5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjmR9UuCHjZd",
        "outputId": "c47a5860-6349-48e3-acdf-fed250051f91"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "65 unique characters\n"
          ]
        }
      ],
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ka__xyDsHjZe",
        "outputId": "f33794c9-a12d-4d81-beba-b8fb33e5a265"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# memisahkan karakter-karakter dalam daftar teks menggunakan tf.strings.unicode_split. \n",
        "# mengambil daftar teks sebagai input dan memisahkannya menjadi karakter-karakter Unicode.\n",
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "B4ZvePG1HjZg"
      },
      "outputs": [],
      "source": [
        "# membuat sebuah lapisan StringLookup menggunakan TensorFlow. \n",
        "# Lapisan ini digunakan untuk mengonversi karakter-karakter dalam teks menjadi ID numerik berdasarkan vocabulary yang diberikan. \n",
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWTDWDGhHjZh",
        "outputId": "7864ff76-025e-4719-ad46-a68a52f18928"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# menggunakan lapisan StringLookup (ids_from_chars) untuk mengonversi karakter-karakter dalam teks (yang telah dipecah menjadi karakter-karakter Unicode) \n",
        "# menjadi ID numerik berdasarkan vocabulary yang telah Anda sediakan.\n",
        "ids = ids_from_chars(chars)\n",
        "ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "-a5sTQbPHjZj"
      },
      "outputs": [],
      "source": [
        "# membuat lapisan StringLookup lain menggunakan TensorFlow. \n",
        "# Lapisan ini digunakan untuk mengonversi ID numerik kembali menjadi karakter-karakter teks asli berdasarkan vocabulary yang telah Anda definisikan sebelumnya. \n",
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I6upQ2qHjZk",
        "outputId": "e62505d3-6ebf-4371-baf6-324b211fd6fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#menggunakan lapisan chars_from_ids untuk mengonversi ID numerik (dalam tensor ids) kembali ke karakter-karakter teks. \n",
        "chars = chars_from_ids(ids)\n",
        "chars"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iL4AQUuHjZm",
        "outputId": "c101e980-ddb5-4dd2-bf0a-e65feb4bd14d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# menggunakan TensorFlow (tf.strings.reduce_join) untuk menggabungkan karakter-karakter dalam tensor chars kembali menjadi teks tunggal.\n",
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "omtQRyAfHjZn"
      },
      "outputs": [],
      "source": [
        "# mengambil ID numerik dan mengonversinya kembali menjadi teks menggunakan lapisan chars_from_ids \n",
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrVUl4P-HjZo",
        "outputId": "43670e24-c6fa-485f-9b37-df0878b84d41"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# menggunakan lapisan ids_from_chars untuk mengonversi karakter-karakter dalam teks yang dipecah menggunakan tf.strings.unicode_split menjadi ID numerik.\n",
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "w8bt8UYgHjZp"
      },
      "outputs": [],
      "source": [
        "# membuat sebuah objek tf.data.Dataset menggunakan tf.data.Dataset.from_tensor_slices dari tensor all_ids.\n",
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjPfdUVnHjZp",
        "outputId": "7286d12d-da7d-48a1-8a07-a9928f046ae2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ],
      "source": [
        "# menggunakan dataset ids_dataset untuk mengambil 10 elemen pertama dari dataset dan kemudian mengonversi ID numerik kembali ke teks menggunakan chars_from_ids. \n",
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "sXz8fojyHjZr"
      },
      "outputs": [],
      "source": [
        "# definisi variabel\n",
        "seq_length = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t7oBi2jBHjZr",
        "outputId": "f0f06d3d-9736-4161-8c54-286cdea1e5c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ],
      "source": [
        "# menggunakan dataset ids_dataset yang telah dibuat sebelumnya dan menggabungkannya menjadi urutan (sequence) dengan panjang sebesar seq_length + 1.\n",
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "# menggunakan loop for untuk mengambil 1 urutan pertama (batch) dari dataset sequences.\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lfn4TBVTHjZs",
        "outputId": "d25611f7-1957-4ccd-a2fc-c09ea994f8f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ],
      "source": [
        "# menggunakan dataset sequences yang telah dibuat sebelumnya untuk mengambil 5 urutan pertama dari dataset. \n",
        "# menggunakan fungsi text_from_ids untuk mengonversi ID numerik kembali menjadi teks dari urutan tersebut, dan hasilnya dicetak ke layar.\n",
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MShJ9z2-HjZt"
      },
      "outputs": [],
      "source": [
        "# mengambil sebuah urutan (sequence) sebagai input dan menghasilkan dua keluaran, yaitu teks input dan teks target.\n",
        "def split_input_target(sequence):\n",
        "    # mengambil semua elemen dalam urutan kecuali elemen terakhir. \n",
        "    input_text = sequence[:-1]\n",
        "    # mengambil semua elemen dalam urutan kecuali elemen pertama. \n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7v-CMT2HjZu",
        "outputId": "db7bbb1c-51cb-4f82-b0b1-64af8c1000cb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# memisahkan teks input dan teks target dari urutan karakter \"Tensorflow\".\n",
        "split_input_target(list(\"Tensorflow\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "1q9LzECvHjZu"
      },
      "outputs": [],
      "source": [
        "# menggunakan metode map pada dataset sequences untuk menerapkan fungsi split_input_target pada setiap elemen dalam dataset sequences.\n",
        "# memisahkan teks input dan teks target dari setiap urutan dalam dataset. \n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiycDy61HjZv",
        "outputId": "26431276-f17d-4fef-e89e-62c48bb8ff4c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ],
      "source": [
        "# menggunakan dataset dataset yang telah dibuat sebelumnya untuk mengambil satu pasang contoh (teks input dan teks target) pertama dari dataset.\n",
        "# menggunakan fungsi text_from_ids untuk mengonversi ID numerik kembali menjadi teks dan mencetaknya ke layar.\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGSq2cOCHjZw",
        "outputId": "d7fe41f4-32eb-4594-a175-c46bf73a0fa3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    # menggunakan metode shuffle untuk mengacak urutan elemen dalam dataset.\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    # menggunakan metode batch untuk mengelompokkan elemen-elemen dataset menjadi batch dengan ukuran sebesar BATCH_SIZE. \n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    # menggunakan metode prefetch untuk mengoptimalkan performa pelatihan model.\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "3979cLE3HjZw"
      },
      "outputs": [],
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "mMNP3V3GHjZx"
      },
      "outputs": [],
      "source": [
        "# membuat model\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    # mengkonversi ID numerik menjadi vektor embedding\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    # membuat lapisan GRU (Gated Recurrent Unit) menggunakan tf.keras.layers.GRU. Lapisan ini digunakan untuk model rekurensi.\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    # menghasilkan output yang memiliki dimensi sesuai dengan jumlah kata atau karakter dalam vocabulary.\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  # mendefinisikan perilaku model saat dijalankan\n",
        "  #  mengatur bagaimana input akan melalui lapisan-lapisan model.\n",
        "  # melakukan embedding, menjalankan lapisan GRU, dan menghasilkan output.\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "8uPnQoY7HjZx"
      },
      "outputs": [],
      "source": [
        "model = MyModel(\n",
        "    # Jumlah total kata atau karakter dalam vocabulary yang digunakan oleh model.\n",
        "    vocab_size=vocab_size,\n",
        "    # Dimensi vektor embedding yang akan digunakan oleh lapisan embedding dalam model.\n",
        "    embedding_dim=embedding_dim,\n",
        "    # Jumlah unit dalam lapisan GRU (Gated Recurrent Unit) dalam model.\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lplHX7snHjZy",
        "outputId": "289bcada-4324-4ecf-b532-da28b35aa21a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ],
      "source": [
        "# menggunakan dataset dataset yang telah dipersiapkan sebelumnya untuk mengambil satu batch (batch pertama) dari data latihan.\n",
        "# menjalankan model model pada input_example_batch untuk mendapatkan prediksi model untuk batch tersebut.\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6422qnCHjZz",
        "outputId": "3ffe4cc8-a7aa-43bc-b489-cc2802fa21b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# melihat ringkasan dari arsitektur model, termasuk jumlah parameter dan struktur lapisan.\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "2F475gzNHjZz"
      },
      "outputs": [],
      "source": [
        "# menggunakan tf.random.categorical untuk mengambil sampel acak dari prediksi model yang pertama dalam batch (indeks 0). \n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "# menggunakan tf.squeeze untuk menghilangkan dimensi yang tidak diperlukan (misalnya, menghilangkan dimensi 1 yang dihasilkan oleh tf.random.categorical)\n",
        "# menggunakan .numpy() untuk mengonversi hasilnya ke dalam format numpy array.\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmgTXlZtHjZ0",
        "outputId": "bc33e6e8-7f44-45dd-9b65-ee1643e7a5d7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([53, 19,  1, 60, 49, 30, 36, 65, 57, 39, 59,  8, 58, 12, 65, 60, 59,\n",
              "       45,  1, 52, 36, 29, 10, 46, 56, 65, 47,  7,  1,  7,  0, 43,  8, 22,\n",
              "       15, 46, 32, 42, 43,  4, 43, 56,  3, 25, 30, 52,  1, 51, 13, 38, 50,\n",
              "       56, 43,  7,  0, 18, 32, 42, 18, 47, 20, 30,  4,  3, 17, 25, 26, 54,\n",
              "       17, 59, 22,  5,  7, 21, 38, 38, 31, 53, 10,  7, 19,  5, 31, 59, 34,\n",
              "       12,  0,  7, 48, 26,  5, 36, 20, 31, 36, 39, 58, 15, 26, 56])"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# array yang berisi indeks-indeks kata atau karakter yang telah diambil sebagai sampel acak dari prediksi model\n",
        "sampled_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FME-v5MeHjZ1",
        "outputId": "842f57bb-e394-47e8-9207-c3eed68a2dab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input:\n",
            " b\"some strong purpose, steel'd\\nThe hearts of men, they must perforce have melted\\nAnd barbarism itself \"\n",
            "\n",
            "Next Char Predictions:\n",
            " b'nF\\nujQWzrZt-s;zutf\\nmWP3gqzh,\\n,[UNK]d-IBgScd$dq!LQm\\nl?Ykqd,[UNK]EScEhGQ$!DLMoDtI&,HYYRn3,F&RtU;[UNK],iM&WGRWZsBMq'\n"
          ]
        }
      ],
      "source": [
        "# mencetak teks input dari batch pertama dalam dataset. \n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "# mencetak prediksi teks berikutnya yang dihasilkan oleh model\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kKX718FJHjZ1"
      },
      "outputs": [],
      "source": [
        "# menggunakan fungsi kerugian SparseCategoricalCrossentropy dari TensorFlow. \n",
        "# Ini adalah salah satu fungsi kerugian yang umum digunakan dalam tugas klasifikasi, terutama saat bekerja dengan data yang tidak dalam format one-hot encoding.\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "61465kxrHjZ2",
        "outputId": "797eac91-b976-487d-b7b1-94702f4b8a24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190623, shape=(), dtype=float32)\n"
          ]
        }
      ],
      "source": [
        "# menggunakan fungsi kerugian loss yang telah didefinisikan sebelumnya untuk menghitung kerugian model pada batch contoh yang diberikan.\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hln2OeR9HjZ3",
        "outputId": "6b1dbbf6-2c45-4c1f-8593-0278553f5282"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "66.06393"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# perhitungan untuk menghitung eksponensial (nilai e pangkat x) dari nilai rata-rata kerugian yang telah dihitung sebelumnya.\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "4BN4KDdyHjZ4"
      },
      "outputs": [],
      "source": [
        "# menggunakan optimizer Adam sebagai optimizer yang akan digunakan dalam pelatihan model.\n",
        "# mengatur fungsi kerugian yang telah Anda definisikan sebelumnya (dalam hal ini, loss) sebagai fungsi kerugian yang akan digunakan selama pelatihan model.\n",
        "model.compile(optimizer='adam', loss=loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "j3vTz39vHjZ5"
      },
      "outputs": [],
      "source": [
        "# menyiapkan konfigurasi untuk menyimpan checkpoint model selama proses pelatihan.\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "# menyimpan checkpoint model selama pelatihan, sesuai dengan konfigurasi yang telah diatur sebelumnya.\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "3pb9ozdrHjZ5"
      },
      "outputs": [],
      "source": [
        "# melatih model Anda selama 20 epoch\n",
        "EPOCHS = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZmg08XtHjZ6",
        "outputId": "4053cf85-2a16-493b-e808-311101d6c3a7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "172/172 [==============================] - 21s 61ms/step - loss: 2.7375\n",
            "Epoch 2/20\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 2.0029\n",
            "Epoch 3/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.7219\n",
            "Epoch 4/20\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.5587\n",
            "Epoch 5/20\n",
            "172/172 [==============================] - 11s 54ms/step - loss: 1.4574\n",
            "Epoch 6/20\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3894\n",
            "Epoch 7/20\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.3353\n",
            "Epoch 8/20\n",
            "172/172 [==============================] - 12s 56ms/step - loss: 1.2918\n",
            "Epoch 9/20\n",
            "172/172 [==============================] - 13s 57ms/step - loss: 1.2510\n",
            "Epoch 10/20\n",
            "172/172 [==============================] - 13s 58ms/step - loss: 1.2120\n",
            "Epoch 11/20\n",
            "172/172 [==============================] - 13s 59ms/step - loss: 1.1731\n",
            "Epoch 12/20\n",
            "172/172 [==============================] - 13s 61ms/step - loss: 1.1324\n",
            "Epoch 13/20\n",
            "172/172 [==============================] - 14s 62ms/step - loss: 1.0911\n",
            "Epoch 14/20\n",
            "172/172 [==============================] - 13s 62ms/step - loss: 1.0456\n",
            "Epoch 15/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.9993\n",
            "Epoch 16/20\n",
            "172/172 [==============================] - 12s 60ms/step - loss: 0.9484\n",
            "Epoch 17/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.8972\n",
            "Epoch 18/20\n",
            "172/172 [==============================] - 12s 59ms/step - loss: 0.8454\n",
            "Epoch 19/20\n",
            "172/172 [==============================] - 12s 61ms/step - loss: 0.7939\n",
            "Epoch 20/20\n",
            "172/172 [==============================] - 12s 62ms/step - loss: 0.7436\n"
          ]
        }
      ],
      "source": [
        "# model Anda akan melalui 20 epoch pelatihan, dan pada setiap akhir epoch, checkpoint model akan disimpan di direktori training_checkpoints.\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "QULUmrg1HjZ7"
      },
      "outputs": [],
      "source": [
        "# melakukan generasi teks satu langkah pada suatu waktu (one-step generation) dengan model pemodelan bahasa.\n",
        "class OneStep(tf.keras.Model):\n",
        "  # metode konstruktor yang digunakan untuk inisialisasi objek OneStep.\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "vwDTUqAJHjab"
      },
      "outputs": [],
      "source": [
        "# melakukan generasi teks satu langkah pada suatu waktu (one-step generation).\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8bEkRGjSHjac",
        "outputId": "0ca3f2b8-37b4-414e-eb56-86aa96d48862"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Thou dost told me in madam. A stuff it off,\n",
            "Having more free forth her with the right, and sleep\n",
            "That hap more man haz but in the flint;\n",
            "Hust to be seld-men'd in all night,\n",
            "And each our body summer fail, myself alone;\n",
            "Her business to be set mock'd now to depose\n",
            "Ye: or the very need of men.\n",
            "But curlina scrutch, my lord, thus I'll come you of mine.\n",
            "\n",
            "KING RICHARD III:\n",
            "Then call it. Do there be the father and to thanks for from him: if\n",
            "Thou speak'st, and staptly, pardon alive.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Axity, Somerset, contemn'd to Henry,\n",
            "But they have letters much: but sue\n",
            "The trumpets, and opposed in Christian seal,\n",
            "Have caused, it will be more than seen his tears:\n",
            "Which torment me, be but unquiet wrench,\n",
            "And throw'd murderer work, deflect I have some leggar;\n",
            "Take ond the accuser at friend,\n",
            "Have so defear'd that you are happied inform any further\n",
            "Where some sent o' the all encounter champ,\n",
            "Think in our brother in Bowit; for my seat contended home,\n",
            "One rest man's ventulive rover so so done,\n",
            "As ' \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 4.229995250701904\n"
          ]
        }
      ],
      "source": [
        "# mengukur waktu mulai proses generasi teks.\n",
        "start = time.time()\n",
        "# mengindikasikan bahwa tidak ada state model sebelumnya yang digunakan.\n",
        "states = None\n",
        "# eks awal 'ROMEO:' diinisialisasi sebagai tensor konstanta next_char.\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "# mengulang sebanyak 1000 karakter untuk generasi teks.\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "# menggabungkan semua karakter yang dihasilkan menjadi satu teks.\n",
        "result = tf.strings.join(result)\n",
        "# mengukur waktu akhir proses generasi teks.\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1fi2Wth2Hjad",
        "outputId": "77dfc35b-aef7-49ea-9fdf-447dd22e44cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThat hath been still'd my behalf.\\n\\nLEONTES:\\nO,\\nsee, the man! why, madame! how shall you be so? And\\nWas't moved him your face?\\nOr, rough by babes? O woful good deny!\\n\\nTRANIO:\\nGo bear the law usurpin; and therefore hear our maideninable,\\nAnd lip is good on thee.\\nHere, sirrah, let's follow to the Duke of Norfolk;\\nAnd strains of earnestry. O citizens comes troud me?\\n\\nThird Servant:\\nUp the man for this force. But, let's be not a\\nMarcius, Selford, bold in person, noble\\nMaligious awake, an argosy-foil\\nHath anger best in with a flower-on's womb,\\nWhile we have shed with Gloucestershire:\\nThen wast nothing out for that, my soul! come on,\\nAnd from the lead on her slave, rotally at safe\\ngenerally agoin, nothing but so:\\nThe noble would have springs my mother doth more\\nsafely, with all speed, so much blood and bound.\\nAnd this, poor seat, swame, I see Queen Margaret say so,\\nThat makes these valour as myself.\\nAnd single sir, your occunation I may, when\\nWast more the palener, frighted stin, and bending\"\n",
            " b\"ROMEO:\\nThou dost thou speak'st there a man not have against no more.\\n\\nARIEL:\\nHa!\\n\\nBUCKINGHAM:\\nWhat must I have stay'd before these will.\\n\\nCORIOLANUS:\\nSay, you never speak?\\n\\nHORTENSIO:\\nSir, I am adoin\\nFor sickness and brings our obnice: nor sound,\\nI call'd thee and cradle men to rouble plain\\nShall shed them forth of sovereignty.\\n\\nPRINCE:\\nLook how my leht be substantia, 'mongst\\nCulling him, hopioving not me forth\\nInfeeth a part from her: but if a more hang in\\nWhat else had been: they mute my injustice strike\\nThe measure of your company, I do not\\nTo be spent.\\n\\nPROSPERO:\\nHow! trumberly?\\n\\nBENVOLIO:\\nAt the case!\\n\\nPOMPEY:\\nTreason, for my father, brove betrays my brother;\\nOur dickness of Harrant comes to fond, and nowly desire\\nOt ends we will acquies a metrecome.\\n\\nLord Mayor:\\nBut, more of this. Canse thou bear it?\\n\\nQUEEN ELIZABETH:\\nO, but he's seen in availous exchance\\nBut brings a curst and always fair of it.\\n\\nKING EDWARD IV:\\nAway with her, forward, swain to thy master.\\nThou that better, worthy sla\"\n",
            " b\"ROMEO:\\nI go, my driar lade me.\\n\\nANGELO:\\nGo sleep; which is the prince my death is most ununta'l, and\\nYour knees gone, tell talest thou not colduce\\nhere: but nothing mark me in my soul's feat: for in your\\nvice, and see how our brother is the begain and sweet'st.\\nThere's one thousands, sorrows like a crybalo,\\nWith all the hapless beekness fast, very 'ecounce,\\nOut of the monal's oft more words of tears\\nAnd sign'd from all my brays farms, work of men, their poor book,\\nBut O, the term that comes to acquit for the curpels.\\nO Jannard! for a very look on me,\\nAnd I am ready to look into a father:\\nThat, art thou set obd men fled at:\\nSpur, they would, he hath like mine about my cousins,\\nWhilst I, with this sepulchre, for the motesty\\nAs you have slain my apparel,\\nIn the rebellion of my forest messess.\\nThey are but bodies and slaves that office\\nBe slain in his natural apperil;\\nBearing will I be welcome but the lord: I am not.\\nMark, farshol, would I do through the corn,\\nAy, fearing my soul is arrivabes.\\n\\n\"\n",
            " b\"ROMEO:\\nWhat says my dear? Emell 'ato, for aught I knew by him, wherein\\nThey is elsewhere to be crodged, and make\\nYour kingness mock'd with hath causes there.\\nGood morrow. We are business in Corioli!\\nBe thou afflict me to the Tunes, and so indirect my\\nfather and so bold:\\nMy father's hand to team or twice: said to your hapsant\\nAntoning him to receiving him:\\nThe rarest shriek before undertends Dire our soldier;\\nYet follow thee have taper'd of a devilish splier\\nFroth hangs upon to thee, ere we shall\\nstrike his captives; believe it.\\n\\nSAMPSON:\\nMy gracious liege, it sting to hear: all sense is\\nthe places of the father visit the reard:\\nAnd thou the sky's absence die too late.\\n\\nAll:\\nCome and welcome to your cape, sir.\\n\\nLUCIO:\\nI waveriary denials, my boy's lips in earness that\\nFive innocents and banish'd man's.\\n\\nKING RICHARD III:\\nHow?\\n\\nGREMIO:\\nGood morrow, neighbour youth.\\n\\nFRIAR THOMAS:\\nYe are not, sir?\\n\\nPOMPHEY:\\nSo do I know there was she to the bowest since\\nBe send anon Rutland's declipers.\\nSome ho\"\n",
            " b\"ROMEO:\\nBelieve me, love, that it speak me too.\\n\\nISABELLA:\\nHe needs muse accept me in my earth,\\nAnd from my chice of branchless wroughth dreams.\\nBut, speak most fitly, I must knock't\\nUnplaced in any else day of condemn'd\\nUpon summerseath thy life, she art as even\\nnot to repair of speaked in an unwillingness\\nNature on his eyes to execution, whom they kiss the sun.\\nAy, this is Trouban, what I have supposed with\\nhim to this match.\\n\\nPERDITA:\\nO Lancaster, as he beams, according souls\\nIs more than can mock nothing.\\nBut, What is the matter? were heaven, how the mutinous pities are not,\\nAnd duttle by the king and Deceive eyes\\nTo them me impedance. Thou art not name that True and blows\\nCry more insence at action,\\nBut nothing can that shame even used us,\\nAnd I but, repair whistle sister, stay.\\n\\nCAPULET:\\nSwear nor hermand! but, sufferent as you\\nTo bring this sent command for her to command;\\nWhich Takes drowned in Plantagenet.\\nCome on, or else the wound that back't will be not\\nFrom the former keeper of f\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.4696266651153564\n"
          ]
        }
      ],
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "# menginisialisasi lima teks awal 'ROMEO:' sebagai tensor konstanta next_char.\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  # menghasilkan karakter berikutnya berdasarkan next_char dan state model sebelumnya, dan kemudian memperbarui state model.\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "# menggabungkan semua karakter yang dihasilkan menjadi satu teks panjang.\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTaOlzuAHjad",
        "outputId": "99fdfae1-d45c-4953-d2a4-55e1f46a85a6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7efc5852ed10>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ],
      "source": [
        "# menggunakan tf.saved_model.save untuk menyimpan model one_step_model ke dalam direktori 'one_step'.\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "# memuat kembali model yang telah disimpan dengan menggunakan tf.saved_model.load. \n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4qPVRCHHjae",
        "outputId": "8ec56627-6429-471e-9a5e-1ec7ac8901bd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROMEO:\n",
            "Good morrow, next way hath mine own;\n",
            "Whilst we shall not in the consulves, myself am all;\n",
            "Doth so m\n"
          ]
        }
      ],
      "source": [
        "# menggunakan model yang telah dimuat kembali, yaitu one_step_reloaded, \n",
        "# untuk melakukan generasi teks dengan teks awal 'ROMEO:'.\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TUGAS\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ebNOA12AKtOc"
      },
      "outputs": [],
      "source": [
        "# mendefinisikan kelas CustomTraining yang merupakan turunan dari model MyModel.\n",
        "class CustomTraining(MyModel):\n",
        "   # mengompilasi metode train_step menjadi sebuah graph TensorFlow, meningkatkan kinerja dan efisiensi dalam pelatihan model.\n",
        "  @tf.function\n",
        "  # melatih model selama satu langkah pelatihan. \n",
        "  # Metode ini menerima input dalam bentuk pasangan (inputs, labels).\n",
        "  def train_step(self, inputs):\n",
        "   # mendekomposisi input menjadi inputs dan labels, yang mengandung data input dan label yang sesuai\n",
        "      inputs, labels = inputs\n",
        "      # menggunakan tf.GradientTape untuk melacak perhitungan dalam metode ini untuk menghitung gradien loss terhadap parameter-model.\n",
        "      with tf.GradientTape() as tape:\n",
        "         # menghitung prediksi model dengan memanggil metode self(inputs, training=True). \n",
        "         predictions = self(inputs, training=True)\n",
        "         # menghitung loss model dengan memanggil metode self.loss dengan labels dan predictions sebagai argumen.\n",
        "         loss = self.loss(labels, predictions)\n",
        "      # menghitung gradien loss terhadap parameter-model dengan menggunakan tape yang telah DIbuat sebelumnya.\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      # menerapkan gradien ke parameter-model menggunakan optimizer yang telah DIkonfigurasikan sebelumnya.\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "      # mengembalikan dictionary yang berisi loss sebagai metrik yang dapat digunakan untuk memantau performa pelatihan.\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "pgJufeAnKxBn"
      },
      "outputs": [],
      "source": [
        "# Model ini akan digunakan untuk pelatihan dengan logika pelatihan yang telah DIdefinisikan dalam metode train_step yang khusus. \n",
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "f0ZhsAlHKvjE"
      },
      "outputs": [],
      "source": [
        "# mengonfigurasi model untuk pelatihan dengan menggunakan optimizer Adam dan loss function Sparse Categorical Crossentropy.\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmp0UhFuKz5U",
        "outputId": "5d0941c5-d078-4d3d-e607-bfcf88336cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "172/172 [==============================] - 15s 60ms/step - loss: 2.7188\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7efc33e1f2e0>"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# memanggil metode fit pada model Anda untuk melatih model dengan dataset yang DIberikan selama 1 epoch.\n",
        "model.fit(dataset, epochs=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5-46g10K1p7",
        "outputId": "1b5fa7f9-58ec-4405-ba80-860fb967697e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 2.1877\n",
            "Epoch 1 Batch 50 Loss 2.0517\n",
            "Epoch 1 Batch 100 Loss 1.9606\n",
            "Epoch 1 Batch 150 Loss 1.8909\n",
            "\n",
            "Epoch 1 Loss: 1.9897\n",
            "Time taken for 1 epoch 14.36 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.8155\n",
            "Epoch 2 Batch 50 Loss 1.7352\n",
            "Epoch 2 Batch 100 Loss 1.7297\n",
            "Epoch 2 Batch 150 Loss 1.7112\n",
            "\n",
            "Epoch 2 Loss: 1.7142\n",
            "Time taken for 1 epoch 13.31 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5950\n",
            "Epoch 3 Batch 50 Loss 1.5869\n",
            "Epoch 3 Batch 100 Loss 1.5605\n",
            "Epoch 3 Batch 150 Loss 1.5520\n",
            "\n",
            "Epoch 3 Loss: 1.5541\n",
            "Time taken for 1 epoch 12.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.4386\n",
            "Epoch 4 Batch 50 Loss 1.4714\n",
            "Epoch 4 Batch 100 Loss 1.4628\n",
            "Epoch 4 Batch 150 Loss 1.4576\n",
            "\n",
            "Epoch 4 Loss: 1.4551\n",
            "Time taken for 1 epoch 12.51 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3995\n",
            "Epoch 5 Batch 50 Loss 1.3714\n",
            "Epoch 5 Batch 100 Loss 1.3531\n",
            "Epoch 5 Batch 150 Loss 1.4020\n",
            "\n",
            "Epoch 5 Loss: 1.3862\n",
            "Time taken for 1 epoch 12.31 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3768\n",
            "Epoch 6 Batch 50 Loss 1.3474\n",
            "Epoch 6 Batch 100 Loss 1.3142\n",
            "Epoch 6 Batch 150 Loss 1.3509\n",
            "\n",
            "Epoch 6 Loss: 1.3336\n",
            "Time taken for 1 epoch 12.07 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2535\n",
            "Epoch 7 Batch 50 Loss 1.2333\n",
            "Epoch 7 Batch 100 Loss 1.2832\n",
            "Epoch 7 Batch 150 Loss 1.3169\n",
            "\n",
            "Epoch 7 Loss: 1.2893\n",
            "Time taken for 1 epoch 12.35 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.2263\n",
            "Epoch 8 Batch 50 Loss 1.2788\n",
            "Epoch 8 Batch 100 Loss 1.2486\n",
            "Epoch 8 Batch 150 Loss 1.2354\n",
            "\n",
            "Epoch 8 Loss: 1.2479\n",
            "Time taken for 1 epoch 12.50 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.2139\n",
            "Epoch 9 Batch 50 Loss 1.2178\n",
            "Epoch 9 Batch 100 Loss 1.2317\n",
            "Epoch 9 Batch 150 Loss 1.2245\n",
            "\n",
            "Epoch 9 Loss: 1.2086\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1786\n",
            "Epoch 10 Batch 50 Loss 1.1818\n",
            "Epoch 10 Batch 100 Loss 1.1844\n",
            "Epoch 10 Batch 150 Loss 1.1602\n",
            "\n",
            "Epoch 10 Loss: 1.1695\n",
            "Time taken for 1 epoch 20.59 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "# digunakan untuk menghitung rata-rata loss selama pelatihan.\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "# melakukan loop sebanyak EPOCHS (dalam hal ini, 10 epoch).\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    # mengatur ulang state rata-rata loss dengan mean.reset_states().\n",
        "    mean.reset_states()\n",
        "    # memanggil model.train_step([inp, target]) untuk melatih model dengan satu batch data. \n",
        "    # mengupdate rata-rata loss dengan memanggil mean.update_state(logs['loss']).\n",
        "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "        logs = model.train_step([inp, target])\n",
        "        mean.update_state(logs['loss'])\n",
        "\n",
        "        # setiap 50 batch, mencetak informasi seperti nomor epoch, nomor batch, dan loss batch yang dihitung\n",
        "        if batch_n % 50 == 0:\n",
        "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "            print(template)\n",
        "\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "       model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "    # mencetak rata-rata loss selama epoch tersebut, waktu yang diperlukan untuk satu epoch, dan garis pembatas.\n",
        "    print()\n",
        "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "    print(\"_\"*80)\n",
        "\n",
        "# Setelah selesai semua epoch, menyimpan bobot model untuk model terlatih terakhir\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
