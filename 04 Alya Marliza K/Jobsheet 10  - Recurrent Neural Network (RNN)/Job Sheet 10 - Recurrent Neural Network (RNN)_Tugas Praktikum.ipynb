{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Tugas**\n",
        "\n",
        "Prosedur pelatihan pada praktikum 2 merupakan prosedur sederhana, yang tidak memberi Anda banyak kendali. Model ini menggunakan \"teacher-forcing\" yang mencegah prediksi buruk diumpankan kembali ke model, sehingga model tidak pernah belajar untuk pulih dari kesalahan. Jadi, setelah Anda melihat cara menjalankan model secara manual, selanjutnya Anda akan mengimplementasikan custom loop pelatihan. Hal ini memberikan titik awal jika, misalnya, Anda ingin menerapkan pembelajaran kurikulum untuk membantu menstabilkan keluaran open-loop model. Bagian terpenting dari loop pelatihan khusus adalah fungsi langkah pelatihan.\n",
        "\n",
        "Gunakan [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape) untuk men track nilai gradient. Anda dapat mempelajari lebih lanjut tentang pendekatan ini dengan membaca [eager execution guide.](https://www.tensorflow.org/guide/basics)\n",
        "\n",
        "Prosedurnya adalah \"\n",
        "1. Jalankan Model dan hitung loss dengan [tf.GradientTape](https://www.tensorflow.org/api_docs/python/tf/GradientTape).\n",
        "2. Hitung update dan terapkan pada model dengan optimize"
      ],
      "metadata": {
        "id": "fToOQybeNvx9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Jawaban**\n",
        "\n",
        "* Perbandingan dengan Praktikum 2"
      ],
      "metadata": {
        "id": "WSLEu5tRSrOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "metadata": {
        "id": "QrDvI0WuSt8l"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MCRPUOeLS2yi",
        "outputId": "e65023f2-1622-4e0e-f1d6-936d23868ea2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1115394/1115394 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggR90GnsS6Eb",
        "outputId": "233f8b92-8cfe-4377-cda6-8e68b53500ee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yJ8wd30zS9M5",
        "outputId": "0e26ed12-445c-4b94-bc97-4c8877a25b2c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKJkBt-NS9Hg",
        "outputId": "ccf58343-739c-46ca-8245-c128add43614"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print unique characters\n",
        "for char in vocab:\n",
        "    print(char, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lVLqNfXPS9Bz",
        "outputId": "944a3884-c152-4c7a-89ce-0a64aeb232b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ! $ & ' , - . 3 : ; ? A B C D E F G H I J K L M N O P Q R S T U V W X Y Z a b c d e f g h i j k l m n o p q r s t u v w x y z "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IpI2niYJTBqx",
        "outputId": "9a1d6fe2-867f-40ff-f5a6-d06b769adeb8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "nfQHVf7TTGPC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2NeDlZkbTDUI",
        "outputId": "7ae717a1-ce97-40f2-998c-a46676b2118a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "5Z3UL8eRTKmE"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y70EuK5rTLOg",
        "outputId": "bd8345bf-cfde-4c49-af5c-1b8a8b5c8d59"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssX6OqvTTN84",
        "outputId": "97a900c0-6774-436a-8597-711cf94b1f0c"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "    return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "Ch1GS1eFTP3t"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGW4TvVxTRNW",
        "outputId": "49feae34-fb8b-440c-805e-eeaa8b66294c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "oBIf4HRFTRvf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2DU1bsk6TT4d",
        "outputId": "2490a452-e5b3-48a9-e8fb-29cda50f380a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "8AA6iuWdTVK6"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xKeroPbOTWj1",
        "outputId": "83a7122b-9d14-40e8-a081-0f15163aaac5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for seq in sequences.take(5):\n",
        "    print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tN7WhpMtTX1M",
        "outputId": "92333a6c-1237-4645-e388-54e2392cdf27"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "  input_text = sequence[:-1]\n",
        "  target_text = sequence[1:]\n",
        "  return input_text, target_text"
      ],
      "metadata": {
        "id": "Ctctcf_pTY__"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlqVIGJ1Tavs",
        "outputId": "34d79114-7223-4382-a5dc-d55382d58aea"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "rK1QoaoYTbtt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example, target_example in dataset.take(1):\n",
        "  print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "  print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_uuOf7wTdER",
        "outputId": "7c13044b-8760-4fd3-a7f4-0650a874c9d2"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M53nfDhuTdCm",
        "outputId": "9d96e221-9b90-421a-f02e-493b837323c8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "36bkxhUuTdAS"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "UswWXSxHTc-M"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "QSKyJABbTc8F"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f33fyq0-TkyV",
        "outputId": "847d2d6b-3f02-4c34-9e4b-73c319b8192e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0gV2eu_cTktr",
        "outputId": "3db92afc-2d53-4d79-8412-cacce92f2981"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0],num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "V42MrOARTvCZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KRYL9YEaTc6C",
        "outputId": "22f08457-9d22-4859-efe0-ec1de158f7e3"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([13,  9, 63,  9, 61,  9, 37, 56, 47,  0, 45, 17, 40,  3, 15, 64, 52,\n",
              "       56, 44, 16, 51, 28, 42, 36, 30,  9, 54,  1,  8, 30, 21, 12, 16, 46,\n",
              "       56, 41, 12, 49, 42, 28, 26, 12, 60, 54, 10, 41, 47, 27, 42, 19, 40,\n",
              "       42, 24,  7, 49, 12, 24, 31, 20, 15,  0,  3, 35, 25, 42, 57, 62, 58,\n",
              "       28,  6, 26, 45, 22, 22, 49, 56,  3, 46, 27, 28, 60, 43, 45, 18,  5,\n",
              "       48, 10,  3, 48, 24, 31, 16, 32, 20,  4, 11, 10, 19,  2, 24])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PoKoTFebTc3q",
        "outputId": "ffab9dcb-9ae1-4989-db45-9b02cf160ea9"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b\"g's own mouth, thereon\\nHis execution sworn.\\n\\nPOLIXENES:\\nI do believe thee:\\nI saw his heart in 's fac\"\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"?.x.v.Xqh[UNK]fDa!BymqeClOcWQ.o\\n-QH;Cgqb;jcOM;uo3bhNcFacK,j;KRGB[UNK]!VLcrwsO'MfIIjq!gNOudfE&i3!iKRCSG$:3F K\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "3qTQ6ypWTw8W"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aXZs-sXvTw5v",
        "outputId": "8ff72e57-b5ed-4369-b5a2-a22e97b9378e"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190138, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bF66xlUvT0Id",
        "outputId": "4be0e4d1-9e28-46cf-c3e5-7d1427a28d89"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.03189"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "bxNn-AGfUEOu"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "T2IL8AogUEHM"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10"
      ],
      "metadata": {
        "id": "Rzox7lr0UGiJ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLUS4GLjUGev",
        "outputId": "7a27f661-ccae-4cd1-f7f7-7200961e2773"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "172/172 [==============================] - 20s 55ms/step - loss: 2.7170\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 10s 51ms/step - loss: 1.9908\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 11s 51ms/step - loss: 1.7099\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 12s 52ms/step - loss: 1.5502\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.4519\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 11s 53ms/step - loss: 1.3850\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 11s 55ms/step - loss: 1.3330\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 12s 54ms/step - loss: 1.2875\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.2466\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 12s 55ms/step - loss: 1.2076\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "TIpvOD9jUOwh"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "AVeLfiB2URJt"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPNa8sZUUUH5",
        "outputId": "a90781ba-237f-4393-c19d-972f41475fce"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Rought that you have you begin, 'O work? hawf you, nor Henry is thine;\n",
            "And change with me now, then she all resolution\n",
            "And bent true London in his son\n",
            "his tomeness of no.\n",
            "\n",
            "GREY:\n",
            "The locker nature in this pity of a name\n",
            "I prithee, rescure her her name of your own,\n",
            "And with death shore. For himself women prince,\n",
            "That by thy heep of Romeo, say therewere?\n",
            "\n",
            "Bo'NON:\n",
            "A dishes all unbonneting.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Meaning, my liene that is all of wincemary of Buckinghty\n",
            "With wisdom in a mother of mony;\n",
            "And so 'nglisber, behold that same, York;\n",
            "While, whose half o'er runs and your counsello,\n",
            "Who is perudua lay my death? and, in the air\n",
            "And bedince would see more fellows in howor. Come on!\n",
            "Let them not water with such burning,\n",
            "Should not infected: at he looks through more\n",
            "mot'st with us\n",
            "And provers to expreselves when we have: But, freery is must\n",
            "Which she hath your heart was love's preced?\n",
            "\n",
            "NORTHUMBERLAND:\n",
            "There is a hair bot! O Gremio, I was comploy\n",
            "I sinner usbrest so find up.\n",
            "\n",
            "TOLINGBAM:\n",
            "Tende \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.250991106033325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogDjCLAUUT9v",
        "outputId": "75d77c1d-2f17-4e60-9254-d6a129e3bb58"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nOur suits, sirrah, go to the time I reib: with all\\nthese travellers up his facter hearted with-hea?'st\\no' the swellence old prosperose, that ourselves or you\\nto presert ox our times Romeo'? thy shepherd.\\n\\nKING HENRY VI:\\nWhy, ho?\\nConstrain!\\n\\nLEONTES:\\nOur earlshing shuts are beaution of hell,\\nTheir mother put in mine\\nOr in the subject villuces; teppet that womun,\\nFrom faith and beg of gaze-morrial,\\nKing Henry last of this? My father's head! most sorrow or restremits,\\nWhich may hear make them things scolding perdence;\\nFor never done spoke conspirach; but those us\\nThanks, than he spake anong.\\n\\nKING LEWIS XI:\\nAhis me not, whenefor Kelsies good strange, dream'd misach\\nAnd pity them spowled the over-raports\\nOf grues of Rome.'\\nBy those should not prove a strange grow;\\nWho hot because ungronsent these tender deserts,\\nAnd peace them father and my master\\nTo this place o' these self-hariture of bost?\\nThou art those that which have a wash on two?\\n\\nKING EDWARD IV:\\nThen hast thou think one shamely p\"\n",
            " b\"ROMEO:\\nPat on the usury with a sightful person;\\nYou, preserve the ground George for Rosaline,\\nAnd three-head of service, look'st thou that it!\\n\\nFirst Senator:\\nAh, by Surrona saves; York;\\nSirrah Sir Grom Rates be patient of soldier\\nThat in us, in arms,\\nWhich neisher thou, till our best are sequence: or they have settled by\\nthe mustering in aul prince's son?\\n\\nCARLIO:\\nSirrah! wife, heavens for patis and both on his misadvented?\\n\\nThird Musician:\\nWhat, ho! affording. Shed\\nShy signished, or who is now kingly being ambluse:\\nBut O, I know it is Bolingbroke by God's nighles\\nHath do protectly owes his name is Edward's souls!\\nWith nothing breathe the Romans, he most igmord\\nThou and when they summous in the mind\\nOf warm at Baptista's tears,\\nAre all my cape. I say, how fares show foothing, good nettle,\\nAs is a man,-gravest-ngear of themselves,\\nI will for gentlewoman, one feed of neck,\\nTo blunt from the king, provosts, asise the other's sleep:\\nAnd to influen haste rescose Bewnock:\\nWhen Thomas Marcius serv\"\n",
            " b\"ROMEO:\\nWhy, they say so?\\n\\nLUCIO:\\nI will not love, sir; I am the mighty progess.\\n\\nShepherd:\\nI never cry out one heavy souch-a rage.\\n\\nPOMPEY:\\nHer master's horse! or given us in heavy! nay, fellow in her remert,\\nLest he not Romeo? when succession,\\nWhom what's the caught old clap, in that through the becking clock\\nUpon my good friend that I am no nore.\\nThe generally lone will king, fier life, I triap,\\n\\nGREMIO:\\nWhat, amen on his hand\\nLet's for me; but the dull't part ta'en,\\nOr something the threstings of his shadu;\\nWhich had fear'd up, longer plasted, helpeth hath besires\\nTheir feature shall be excellence;\\nAnd all my lovely last all things; ands\\nRut redeen thy forth and looks are fruit,\\nHe was Admiracher, wife, not to take all\\nThe garland, bloody wind Christian, nurse.\\n\\nKING LEWIS XI:\\nI wisly, sir. What honour is to me?\\nThe excheel gave o'ly that they do?\\n\\nKING EDWARD IV:\\nSo, news, lords well-graced the gentlewoman,\\nThe subject lives sturgly--this answer I may now;\\nAsise-think I will pass with sw\"\n",
            " b\"ROMEO:\\nThou think'st would what drie should sink me up\\nUntil thou art a furnish'd nor inquirtly\\nThe heavens something great eye-upon.\\nTherefore be gone, so! Is not fail your wife.\\n\\nGRUMIO:\\nAnd so, not ask, he will meet your life,\\nAnd well became thee in father's blood.\\n\\nGLOUCESTER:\\nSound Buckingham, for my natciquies shall bid it out\\nand sun: or, by the whotion turn to lose it seared\\nScolding to nothing, for one as love;\\nBut nivin Hortenso, this it is,\\nBy the not of it is speaking breath upon,\\nThe speed up whom in his cloyness scepteed\\ncoffing into their countross.\\n\\nKING LEWIS XI:\\nWhy, therefore point to have them like some lottering hear\\nTo planted sheephon come of ma? but if you wind,\\nHe was since therewell. Thy hearts the gods in blusfection\\nA quarrel; aschurghy sair charity too, but in little\\nTo the least of keep in feeling king and hardful\\nFrom off; they last no gestivel young weeds done:\\nLook, and press, gentle spenney persons are the brot\\nour master's liar, thou hast thought for mercy\"\n",
            " b\"ROMEO:\\nThen makes the subject soul o' the sea, or Lucio.\\nMore honour of tears this, he cracks, deposed\\nIn sucdure, 'gands and brike Dorncy, and\\nwills meet not. He was not whither well.\\n\\nThird Servant:\\nHere came wept to thy looks? or all.\\n\\nYORK:\\nAs well as Clifford? look upon him:\\nBefall be desperiated, when to the Pompey--these arrested\\nred down thou attand from sure through events,\\nBut o'er-das-pardon.\\n\\nTRANIO:\\n\\nJULIET:\\nCall him he lose himself maiderly, we are.\\nThe die over-ill plegives and Wrange for you.\\nBut now I know you will set down her hundred,\\nThat they once, nor in anough Claudio\\nLover, if they dare here, nor his voices\\nThat break the witnesses---pursitulation wonder days,\\nBut in thy sound doth loving, thus,\\nYou proclaim doar widows here-run, for our tongue,\\nThis is the sea of blood.\\n\\nJULIET:\\nI would he spake, you, is a guifes assisted:\\nAy, and, of Sheeglb.\\n\\nFirst Servant:\\nHow farest thou gates, what dost thou breathes their tooth?\\n\\nKING EDWARD IV:\\nCome, city farther for the time \"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 3.811204433441162\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4tbQiB3URGM",
        "outputId": "0ccfc137-6037-4a1a-ce71-0767b0a4f20e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x7ab17007e140>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gwz9B1w6UeKU",
        "outputId": "7008f214-51ee-43b2-f8a2-25b5fe3e0f59"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "A vixos and true fence an aside comes\n",
            "Than think not with the hignark of news, are more\n",
            "Thousand ni\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "f-FvH1bbntdn"
      },
      "outputs": [],
      "source": [
        "class CustomTraining(MyModel):\n",
        "  @tf.function\n",
        "  def train_step(self, inputs):\n",
        "    inputs, labels = inputs\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = self(inputs, training=True)\n",
        "      loss = self.loss(labels, predictions)\n",
        "      grads = tape.gradient(loss, model.trainable_variables)\n",
        "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "\n",
        "      return {'loss': loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kode diatas menerapkan train_step method sesuai dengan  [Keras' train_step conventions](https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit). Ini opsional, tetapi memungkinkan Anda mengubah perilaku langkah pelatihan dan tetap menggunakan keras [Model.compile](https://www.tensorflow.org/api_docs/python/tf/keras/Model#compile) and [Model.fits](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit) methods."
      ],
      "metadata": {
        "id": "8kwQaYW7Of-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = CustomTraining(\n",
        "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "3xkCHtDtO7IH"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
        "             loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "XYOhi_j3PAGt"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDuRjzoNPCG_",
        "outputId": "48bcf005-05b8-44e7-e602-3420945154fe"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 15s 57ms/step - loss: 2.7164\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab17007ffa0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(dataset, epochs=1)"
      ],
      "metadata": {
        "id": "Yy04qGo-PFEE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1823b4a0-9d66-4866-b7b8-6d2ecbd98732"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172/172 [==============================] - 11s 55ms/step - loss: 1.9808\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ab1856df280>"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Atau jika ingin lebih mengetahui dalamnya, kita bisa membuat custom training loop sendiri:"
      ],
      "metadata": {
        "id": "erkYCvW1PHZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "mean = tf.metrics.Mean()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  mean.reset_states()\n",
        "  for (batch_n, (inp, target)) in enumerate(dataset):\n",
        "    logs = model.train_step([inp, target])\n",
        "    mean.update_state(logs['loss'])\n",
        "\n",
        "    if batch_n % 50 == 0:\n",
        "      template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
        "      print(template)\n",
        "\n",
        "  # saving (checkpoint) the model every 5 epochs\n",
        "  if (epoch + 1) % 5 == 0:\n",
        "    model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
        "\n",
        "  print()\n",
        "  print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
        "  print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
        "  print(\"_\"*80)\n",
        "\n",
        "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
      ],
      "metadata": {
        "id": "lHvqN5STPH_H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a32461-6009-4529-fafc-b78b4ddb6f50"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 Batch 0 Loss 1.8323\n",
            "Epoch 1 Batch 50 Loss 1.7398\n",
            "Epoch 1 Batch 100 Loss 1.6428\n",
            "Epoch 1 Batch 150 Loss 1.6215\n",
            "\n",
            "Epoch 1 Loss: 1.7043\n",
            "Time taken for 1 epoch 20.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 2 Batch 0 Loss 1.6138\n",
            "Epoch 2 Batch 50 Loss 1.5738\n",
            "Epoch 2 Batch 100 Loss 1.5692\n",
            "Epoch 2 Batch 150 Loss 1.4994\n",
            "\n",
            "Epoch 2 Loss: 1.5454\n",
            "Time taken for 1 epoch 13.31 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 3 Batch 0 Loss 1.5263\n",
            "Epoch 3 Batch 50 Loss 1.4357\n",
            "Epoch 3 Batch 100 Loss 1.4820\n",
            "Epoch 3 Batch 150 Loss 1.4541\n",
            "\n",
            "Epoch 3 Loss: 1.4479\n",
            "Time taken for 1 epoch 20.49 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 4 Batch 0 Loss 1.3863\n",
            "Epoch 4 Batch 50 Loss 1.4196\n",
            "Epoch 4 Batch 100 Loss 1.3597\n",
            "Epoch 4 Batch 150 Loss 1.3539\n",
            "\n",
            "Epoch 4 Loss: 1.3801\n",
            "Time taken for 1 epoch 10.96 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 5 Batch 0 Loss 1.3163\n",
            "Epoch 5 Batch 50 Loss 1.3056\n",
            "Epoch 5 Batch 100 Loss 1.3256\n",
            "Epoch 5 Batch 150 Loss 1.3213\n",
            "\n",
            "Epoch 5 Loss: 1.3288\n",
            "Time taken for 1 epoch 11.27 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 6 Batch 0 Loss 1.3185\n",
            "Epoch 6 Batch 50 Loss 1.2617\n",
            "Epoch 6 Batch 100 Loss 1.3070\n",
            "Epoch 6 Batch 150 Loss 1.2995\n",
            "\n",
            "Epoch 6 Loss: 1.2828\n",
            "Time taken for 1 epoch 11.26 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 7 Batch 0 Loss 1.2552\n",
            "Epoch 7 Batch 50 Loss 1.2464\n",
            "Epoch 7 Batch 100 Loss 1.2264\n",
            "Epoch 7 Batch 150 Loss 1.2389\n",
            "\n",
            "Epoch 7 Loss: 1.2420\n",
            "Time taken for 1 epoch 10.94 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 8 Batch 0 Loss 1.1804\n",
            "Epoch 8 Batch 50 Loss 1.1798\n",
            "Epoch 8 Batch 100 Loss 1.2005\n",
            "Epoch 8 Batch 150 Loss 1.1560\n",
            "\n",
            "Epoch 8 Loss: 1.2026\n",
            "Time taken for 1 epoch 12.10 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 9 Batch 0 Loss 1.1131\n",
            "Epoch 9 Batch 50 Loss 1.1442\n",
            "Epoch 9 Batch 100 Loss 1.1913\n",
            "Epoch 9 Batch 150 Loss 1.1724\n",
            "\n",
            "Epoch 9 Loss: 1.1625\n",
            "Time taken for 1 epoch 11.47 sec\n",
            "________________________________________________________________________________\n",
            "Epoch 10 Batch 0 Loss 1.1143\n",
            "Epoch 10 Batch 50 Loss 1.0706\n",
            "Epoch 10 Batch 100 Loss 1.1565\n",
            "Epoch 10 Batch 150 Loss 1.1291\n",
            "\n",
            "Epoch 10 Loss: 1.1215\n",
            "Time taken for 1 epoch 12.26 sec\n",
            "________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Jalankan kode diatas dan sebutkan perbedaanya dengan praktikum 2?\n",
        "\n",
        "* Perbedaan diatas dengan Praktikum 2 ialah:\n",
        "\n",
        "**prosedur pelatihan**\n",
        "\n",
        "Pada praktikum 2, digunakan pendekatan pelatihan yang lebih sederhana dan umum dengan menggunakan fungsi model.fit. Sebaliknya, pada kode tugas, terdapat penerapan pendekatan pelatihan yang lebih spesifik dan kompleks. Pendekatan ini melibatkan definisi metode train_step dalam model turunan yang mengatur pelatihan pada tingkat batch. Di dalam train_step, dilakukan secara eksplisit perhitungan loss, perhitungan gradien, dan penerapan pembaruan bobot model dengan menggunakan apply_gradients. Selain itu, juga digunakan objek tf.metrics.Mean untuk menghitung rata-rata loss selama proses pelatihan.\n",
        "\n",
        "Pendekatan yang diterapkan pada tugas memberikan lebih banyak kontrol dan fleksibilitas dalam mengatur proses pelatihan model. Hal ini memungkinkan untuk menyesuaikan langkah-langkah pelatihan sesuai kebutuhan spesifik, dan memungkinkan penggunaan metrik tambahan selain loss untuk pemantauan kinerja model."
      ],
      "metadata": {
        "id": "biyq4vl4PagP"
      }
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}